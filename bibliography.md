---
layout: default
img: attentionmodel
img_link: "http://arxiv.org/abs/1409.0473"
caption: "Bahdanau et al (ICLR 2015) describe an attention model which uses a soft word alignment as part of a Recursive Neural Network based Encoder-Decoder architecture for Neural Machine Translation."
title: Bibliography
active_tab: bib
---

## Bibliography

Also see [References from the ACL 2016 Tutorial](https://sites.google.com/site/acl16nmt/home/references) by Luong, Cho and Manning.

### Recurrent Neural Networks

1. [Hochreiter & Schmidhuber, 1997](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf). Long Short-term Memory. 
1. [Bengio et al, IEEE Trans on Neural Networks](http://www.dsi.unifi.it/~paolo/ps/tnn-94-gradient.pdf). Learning long-term dependencies with gradient descent is difficult.
1. [Mikael Bodén 2002]({{ site.baseurl }}/assets/cached/boden-rnn-techrept.pdf). A guide to recurrent neural networks and backpropagation.
1. [Graves et al, ICML 2006](http://www.cs.toronto.edu/~graves/icml_2006.pdf). Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks.

### RNN Language models

1. [Mikolov et al InterSpeech 2010](http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf). Recurrent neural network based language model.
1. [Mikolov PhD thesis](http://www.fit.vutbr.cz/~imikolov/rnnlm/thesis.pdf). Statistical Language Models based on Neural Networks.
1. [Mikolov et al ICASSP 2011](http://www.fit.vutbr.cz/research/groups/speech/publi/2011/mikolov_icassp2011_presentation_rnnlm-extension.pdf). Extensions of recurrent neural network language model.
1. [Zoph, Vaswani, May, Knight, NAACL'16](http://www.isi.edu/natural-language/mt/simple-fast-noise.pdf). Simple, Fast Noise Contrastive Estimation for Large RNN Vocabularies. 
1. [Ji, Vishwanathan, Satish, Anderson, Dubey, ICLR'16](http://arxiv.org/pdf/1511.06909.pdf). BlackOut: Speeding up Recurrent Neural Network Language Models with very Large Vocabularies. 

### n-gram Neural Language models

1. [Bengio, Ducharme, Vincent, Jauvin, JMLR'03](http://www.jmlr.org/papers/v3/bengio03a.html). A Neural Probabilistic Language Model.
1. [Morin & Bengio, AISTATS'05](http://www.iro.umontreal.ca/~lisa/pointeurs/hierarchical-nnlm-aistats05.pdf). Hierarchical Probabilistic Neural Network Language Model. 
1. [Mnih & Hinton, NIPS'09](http://www.cs.toronto.edu/~amnih/papers/hlbl_final.pdf). A Scalable Hierarchical Distributed Language Model. 
1. [Mnih & Teh, ICML'12](http://www.cs.toronto.edu/~amnih/papers/ncelm.pdf). A fast and simple algorithm for training neural probabilistic language models. 
1. [Vaswani, Zhao, Fossum, Chiang, EMNLP'13](http://www.isi.edu/~avaswani/NCE-NPLM.pdf). Decoding with Large-Scale Neural Language Models Improves Translation. 
1. [Kim, Jernite, Sontag, Rush, AAAI'16](http://arxiv.org/pdf/1508.06615.pdf). Character-Aware Neural Language Models. 
1. [Ji, Haffari, Eisenstein, NAACL'16](http://arxiv.org/pdf/1603.01913.pdf). A Latent Variable Recurrent Neural Network for Discourse-Driven Language Models. 
1. [Wang, Cho, ACL'16](http://aclweb.org/anthology/P/P16/P16-1125.pdf). Larger-Context Language Modelling with Recurrent Neural Network. 

### Neural Machine Translation

1. [Bahdanau et al., ICLR'15](http://arxiv.org/pdf/1409.0473.pdf). Neural Translation by Jointly Learning to Align and Translate. 
1. [Chung, Cho, Bengio, ACL'16](http://arxiv.org/pdf/1603.06147.pdf). A Character-Level Decoder without Explicit Segmentation for Neural Machine Translation. 
1. [Cohn, Hoang, Vymolova, Yao, Dyer, Haffari, NAACL'16](http://arxiv.org/pdf/1601.01085.pdf). Incorporating Structural Alignment Biases into an Attentional Neural Translation Model. 
1. [Gu, Lu, Li, Li, ACL'16](http://arxiv.org/pdf/1603.06393.pdf). Incorporating Copying Mechanism in Sequence-to-Sequence Learning. 
1. [Gulcehre, Ahn, Nallapati, Zhou, Bengio, ACL'16](http://arxiv.org/pdf/1603.08148.pdf). Pointing the Unknown Words. 
1. [Ling, Luís, Marujo, Astudillo, Amir, Dyer, Black, Trancoso, EMNLP'15](http://arxiv.org/pdf/1508.02096.pdf). Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation. 
1. [Luong et al., ACL'15a](http://www.aclweb.org/anthology/P15-1002). Addressing the Rare Word Problem in Neural Machine Translation. 
1. [Luong et al., ACL'15b](http://aclweb.org/anthology/D/D15/D15-1166.pdf). Effective Approaches to Attention-based Neural Machine Translation. 
1. [Luong & Manning, IWSLT'15](http://nlp.stanford.edu/pubs/luong-manning-iwslt15.pdf). Stanford Neural Machine Translation Systems for Spoken Language Domain. 
1. [Sennrich, Haddow, Birch, ACL'16a](http://arxiv.org/pdf/1511.06709.pdf). Improving Neural Machine Translation Models with Monolingual Data. 
1. [Sennrich, Haddow, Birch, ACL'16b](http://arxiv.org/pdf/1508.07909.pdf). Neural Machine Translation of Rare Words with Subword Units. 
1. [Tu, Lu, Liu, Liu, Li, ACL'16](http://arxiv.org/pdf/1601.04811.pdf). Modeling Coverage for Neural Machine Translation. 

### Encoder-Decoder Neural Networks

1. [Mnih et al., NIPS'14](http://papers.nips.cc/paper/5542-recurrent-models-of-visual-attention.pdf) Recurrent Models of Visual Attention. 
1. [Sutskever et al., NIPS'14](http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf). Sequence to Sequence Learning with Neural Networks. 
1. [Xu, Ba, Kiros, Cho, Courville, Salakhutdinov, Zemel, Bengio, ICML'15](http://jmlr.org/proceedings/papers/v37/xuc15.pdf). Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. 
1. [Jia, Liang, ACL'16](http://arxiv.org/pdf/1606.03622.pdf). Data Recombination for Neural Semantic Parsing. 

### Multi-lingual Neural MT

1. [Zoph, Knight, NAACL'16](http://www.isi.edu/natural-language/mt/multi-source-neural.pdf). Multi-source neural translation. 
1. [Dong, Wu, He, Yu, Wang, ACL'15](http://www.aclweb.org/anthology/P15-1166). Multi-task learning for multiple language translation. 
1. [Firat, Cho, Bengio, NAACL'16](http://arxiv.org/pdf/1601.01073.pdf). Multi-Way, Multilingual Neural Machine Translation with a Shared Attention Mechanism. 

### Neural Word Alignment

1. [Yang et al, ACL 2013](http://www.aclweb.org/anthology/P13-1017.pdf). Word Alignment Modeling with Context Dependent Deep Neural Network.
1. [Akihiro Tamura, Taro Watanabe and Eiichiro Sumita ACL 2014](http://anthology.aclweb.org/P/P14/P14-1138.pdf). Recurrent Neural Networks for Word Alignment Model.

