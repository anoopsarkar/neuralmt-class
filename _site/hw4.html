<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SFU NLP class: Homework 4 | Decoding</title>

    <!-- CSS -->
    <link href="dist/css/bootstrap.min.css" rel="stylesheet">
    <link href="dist/css/bootstrap-glyphicons.css" rel="stylesheet">
    <link href="assets/css/nlp-class.css" rel="stylesheet">   
    <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">

    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>

    <!-- MathJax -->
    <script type="text/javascript"
      src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
  </head>
  <body>
    <a class="sr-only" href="#content">Skip to main content</a>

    <!-- Docs master nav -->
    <header class="navbar navbar-fixed-top navbar-default" role="banner">
      <div class="container">
        <div class="row">
        <div class="navbar-header">
          <button class="navbar-toggle" type="button" data-toggle="collapse" data-target=".navbar-collapse">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <ul class="nav navbar-nav">
            <li id="main_page"><a href="index.html" class="navbar-brand">Natural Language Processing</a></li>
          </ul>
        </div>
        <nav class="collapse navbar-collapse" role="navigation">
          <ul class="nav navbar-nav">
            <li id="syllabus"><a href="syllabus.html">Syllabus</a></li>
            <li id="homework">
              <a class="dropdown-toggle" data-toggle="dropdown" href="#">Homework <span class="caret"></span></a>
              <ol class="dropdown-menu">
                <li><a href="hw0.html">0. Setup</a></li>
                <li><a href="hw1.html">1. Word Segmentation</a></li>
                <li><a href="hw2.html">2. Phrasal Chunking</a></li>
                <li><a href="hw3.html">3. Word Alignment</a></li>
                <li><a href="hw4.html">4. Translation Decoding</a></li>
                <li><a href="hw5.html">5. Translation Reranking</a></li>
              </ol>
            </li>
            <li id="leaderboard"><a href="leaderboard.html">Leaderboard</a></li>
            <li id="project"><a href="project.html">Project</a></li>
            <li id="faq"><a href="faq.html">FAQ</a></li>
          </ul>
        </nav>
      </div>
      </div>
    </header>

    <div class="container">
      <div class="row">
        <div class="col-sm-2 hidden-sm hidden-xs">
          <a href="http://en.wikipedia.org/wiki/Voynich_manuscript"> 
          <img src="assets/img/voynich.jpg" class="img-responsive img-rounded" alt=""/>
          </a> 
          <span class="text-muted"><i>The Voynich manuscript</i></span>
        </div>
        <div class="col-sm-10">
          <h1 id="translation-decoding-span-classtext-mutedhomework-4span">Translation Decoding <span class="text-muted">Homework 4</span></h1>

<p class="text-muted">Due on Tuesday, November 8, 2016</p>

<p>Decoding is process of taking input in French:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>honorables sénateurs , que se est - il passé ici , mardi dernier ?
</code></pre>
</div>

<p>…And finding its best English translation under your  model:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>honourable senators , what happened here last Tuesday ?
</code></pre>
</div>

<p>To decode, we need a model of English sentences conditioned on the
French sentence. You did most of the work of creating
such a model in <a href="hw1.html">Homework 1</a>. In this assignment,
we will give you some French sentences and a probabilistic model consisting of
a phrase-based translation model <script type="math/tex">\Pr_{\textrm{TM}}(\textbf{f},\textbf{a} \mid \textbf{e})</script>
and an n-gram language model <script type="math/tex">\Pr_{\textrm{LM}}(\textbf{e})</script>. <strong>Your 
challenge is to find the most probable English translation under 
the model.</strong> We assume a noisy channel decomposition.</p>

<center>
$$\begin{align*} \textbf{e}^* &amp; = \arg \max_{\textbf{e}} \Pr(\textbf{e} \mid \textbf{f}) \\ 
&amp; = \arg \max_{\textbf{e}} \frac{\Pr_{\textrm{TM}}(\textbf{f} \mid \textbf{e}) \times \Pr_{\textrm{LM}}(\textbf{e})}{\Pr(\textbf{f})} \\ 
&amp;= \arg \max_{\textbf{e}} \Pr_{\textrm{TM}}(\textbf{f} \mid \textbf{e}) \times \Pr_{\textrm{LM}}(\textbf{e}) \\ 
&amp;= \arg \max_{\textbf{e}} \sum_{\textbf{a}} \Pr_{\textrm{TM}}(\textbf{f},\textbf{a} \mid \textbf{e}) \times \Pr_{\textrm{LM}}(\textbf{e}) \end{align*}$$
</center>

<h2 id="getting-started">Getting Started</h2>

<p>You must have git and python (2.7) on your system to run the
assignments.</p>

<p>If you have already cloned the <code class="highlighter-rouge">nlp-class-hw</code> repository,
then do the following to get the files for Homework 3:</p>

<div class="highlighter-rouge"><pre class="highlight"><code># go to the directory where you did a git clone before
cd nlp-class-hw
git pull origin master
</code></pre>
</div>

<p>Or you can create a new directory that does a fresh clone of the
repository:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>git clone https://github.com/anoopsarkar/nlp-class-hw.git
</code></pre>
</div>

<p>In the <code class="highlighter-rouge">decoder</code> directory you will find several python programs
and data sets that you will use for this assignment.</p>

<p><code class="highlighter-rouge">default.py</code> contains the default decoder:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>python default.py &gt; output
</code></pre>
</div>

<p><code class="highlighter-rouge">default.py</code> implements a complete but very simple decoder. The
above command creates the file <code class="highlighter-rouge">output</code> with translations of
<code class="highlighter-rouge">data/input</code>.  You can compute <script type="math/tex">\Pr(\textbf{e} \mid \textbf{f})</script>
using <code class="highlighter-rouge">score-decoder.py</code>.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>python score-decoder.py &lt; output
</code></pre>
</div>

<p>Or you can do it all at once:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>python default.py | python score-decoder.py
</code></pre>
</div>

<h3 id="scoring-decoder-output">Scoring decoder output</h3>

<p>The <code class="highlighter-rouge">score-decoder.py</code> command computes the total log probability
for the output file. If there are <script type="math/tex">N</script> sentences in the output:
<script type="math/tex">\textbf{f}^1, \ldots, \textbf{f}^N</script> then the output score is the sum of the
translation and language model log probability over all sentences:</p>

<p>$$\textrm{score} = \sum_{i=1}^N \log \sum_{\textbf{a}} \Pr_{\textrm{TM}}(\textbf{f}^i ,\textbf{a} \mid \textbf{e}^i) \times \Pr_{\textrm{LM}}(\textbf{e}^i) $$</p>

<p>It sums over all possible ways that the model could have generated
the English from the French, including translations that permute
the phrases. This sum is exponential (and hence intractable) in
general, but the phrase dictionary is fixed and sparse (and small
for this homework), so we can compute it in a few minutes. It is
still easier to do this than it is to find the optimal translation
but if you look at the implementation of <code class="highlighter-rouge">score-decoder.py</code> you may
get some hints about how to do the assignment!</p>

<p>The most important thing to understand about the scoring of your
decoder is that we are not scoring accuracy of the output translation,
but rather we are scoring a decoder based on whether the <script type="math/tex">\arg\max</script>
output produced by a decoder has a <em>model score</em> that approaches
the <em>best possible</em> model score. Note that the translation with the
best score may not still be a good translation in the target language.</p>

<h3 id="understanding-the-default-decoder">Understanding the default decoder</h3>

<p>The decoder generates the most probable translations that it can
find, using three common approximations.</p>

<p>First, it seeks the <em>Viterbi approximation</em> to the most probable
translation. Instead of computing the intractable sum over all
alignments for each sentence, we simply find the best single alignment
and use its translation.</p>

<center>$$\begin{align*} \textbf{e}^* &amp;= \arg \max_{\textbf{e}} \sum_{\textbf{a}} \Pr_{\textrm{TM}}(\textbf{f},\textbf{a} \mid \textbf{e}) \times \Pr_{\textrm{LM}}(\textbf{e}) \\ 
&amp;\approx \arg \max_{\textbf{e}} \max_{\textbf{a}} \Pr_{\textrm{TM}}(\textbf{f},\textbf{a} \mid \textbf{e}) \times \Pr_{\textrm{LM}}(\textbf{e}) \end{align*}$$</center>

<p>Second, it translates French phrases into English without changing
their order. So, it only reorders words  if the reordering has been
memorized as a phrase pair.  For example, in the first sentence,
we see that <em><span class="text text-primary">mardi</span> <span class="text text-danger">dernier</span></em> is correctly translated
as <em><span class="text text-danger">last</span> <span class="text text-primary">Tuesday</span></em>.  If we consult <code class="highlighter-rouge">data/tm</code>, we will
find that the model has memorized the phrase pair <code class="highlighter-rouge">mardi dernier
||| last Tuesday</code>. But in the second sentence, we see that <em><span class="text text-danger">Comité</span> <span class="text text-primary">de
sélection</span></em> is translated as <em><span class="text text-danger">committee</span> <span class="text text-primary">selection</span></em>, rather than the more correct <em><span class="text text-primary">selection</span> <span class="text text-danger">committee</span></em>.  To show that this is a search
problem rather than a modeling problem, we can generate the latter
output by hand and check that the model really prefers it.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>head -2 data/input | tail -1 &gt; example
python default.py -i example | python score-decoder.py -i example
echo a selection committee was achievement . | python score-decoder.py -i example
</code></pre>
</div>

<p>Recall that the scores are reported as log-probabilities, and higher
scores (with lower absolute value) are better. We see that the model
prefers <em><span class="text text-primary">selection</span> <span class="text text-danger">committee</span></em>, but the decoder does
not consider this word order.</p>

<p>Finally, our decoder uses strict pruning. As it consumes the input
sentence from left to right, it keeps only the highest-scoring
output up to that point. You can vary the number of number of outputs
kept at each point in the translation using the <code class="highlighter-rouge">-s</code> parameter. See
how this affects the resulting model score.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>python default.py | python score-decoder.py
python default.py -s 10000 | python score-decoder.py
</code></pre>
</div>

<h2 id="the-challenge">The Challenge</h2>

<p>Your task is to <strong>find the most probable English translation</strong>.
Our model assumes that any segmentation of the French sentence into
phrases followed by a one-for-one substitution and permutation of
those phrases is a valid translation. We make the simplifying
assumption that segmentation and ordering probabilities are uniform
across all sentences, hence constant.  This means that
<script type="math/tex">\Pr(\textbf{e},\textbf{a} \mid \textbf{f})</script> is proportional to the
product of the n-gram probabilities in <script type="math/tex">\Pr_{\textrm{LM}}(\textbf{e})</script>
and the phrase translation probabilities in
<script type="math/tex">\Pr_{\textrm{TM}}(\textbf{f},\textbf{a} \mid \textbf{e})</script>. To avoid
numerical underflow we work in logspace, seeking <script type="math/tex">\arg \max_{\textbf{e}}
\max_{\textbf{a}} \log \Pr_{\textrm{TM}}(\textbf{f},\textbf{a} \mid
\textbf{e}) + \log \Pr_{\textrm{LM}}(\textbf{e})</script>. The baseline
decoder works with log probabilities, so you can simply follow what
it does.</p>

<h3 id="the-leaderboard">The Leaderboard</h3>

<p>In this homework, the score produced by <code class="highlighter-rouge">score-decoder.py</code> will be
the same as the score on the leaderboard. So you do not need to
upload your output nearly as often as you did in other homeworks.</p>

<p>To get on the leaderboard, produce your output file:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>python your-decoder.py &gt; output
</code></pre>
</div>

<p>Then upload the file <code class="highlighter-rouge">output</code> to the leaderboard for Homework 4 on
<a href="https://sfu-nlp-class.appspot.com">sfu-nlp-class.appspot.com</a></p>

<h3 id="the-baseline">The Baseline</h3>

<p>At minimum, you must implement a beam-search decoder like the one
we have given you that is also capable of <em>swapping adjacent phrases</em>.
To get full credit, you <strong>must</strong> additionally experiment with another
decoding algorithm.  Any permutation of phrases is a valid translation,
so we strongly suggest searching over all or some part of this
larger space. This search is NP-Hard, so it will not be easy.</p>

<p>A detailed description of the standard algorithm for a beam-search
decoder is provided in the following lecture notes:</p>

<blockquote>
  <p><a href="assets/mcollins-notes/pb.pdf">Phrase-based Translation Models</a>. Michael Collins.</p>
</blockquote>

<h3 id="extending-the-baseline">Extending the baseline</h3>

<p>There are several approaches that tackle the decoding problem for
machine translation:</p>

<ul>
  <li>Use a beam width parameter instead of picking the top-k in each stack as described in page 12 of the <a href="assets/mcollins-notes/pb.pdf">Collins lecture notes</a>.</li>
  <li>Implement a pruning score separate from the model score:
    <ul>
      <li>Use a distortion penalty so that reordering is penalized by distance as described in page 11 of the <a href="assets/mcollins-notes/pb.pdf">Collins lecture notes</a>.</li>
      <li>Use a weight for the language model and translation model and tune the weights (by hand) to minimize search error as described in page 17 of the <a href="http://anoopsarkar.github.io/nlp-class/assets/slides/06-decoding.pdf">Koehn lecture notes on decoding</a>.</li>
      <li>Use future cost as described in pages 25-29 of the <a href="http://anoopsarkar.github.io/nlp-class/assets/slides/06-decoding.pdf">Koehn lecture notes on decoding</a>.</li>
    </ul>
  </li>
  <li><a href="http://www.iro.umontreal.ca/~felipe/bib2webV0.81/cv/papers/paper-tmi-2007.pdf">Implement a greedy decoder</a>.</li>
  <li><a href="http://aclweb.org/anthology-new/W/W01/W01-1408.pdf">Use A* search</a>.</li>
  <li><a href="http://aclweb.org/anthology//D/D13/D13-1022.pdf">Use Lagrangian relaxation</a>. Guaranteed to find the best score!</li>
  <li><a href="http://aclweb.org/anthology-new/P/P09/P09-1038.pdf">Use a traveling salesman problem (TSP) solver</a>.</li>
  <li><a href="http://aclweb.org/anthology-new/N/N09/N09-2002.pdf">Use integer linear programming</a>.</li>
  <li><a href="http://mi.eng.cam.ac.uk/~wjb31/ppubs/ttmjnle.pdf">Use finite-state algorithms</a>.</li>
</ul>

<p>These methods all attempt to approximate or solve the Viterbi
approximation to decoding.  You can also try to approximate
<script type="math/tex">\Pr(\textbf{e} \mid \textbf{f})</script> directly. Here are some attempts
but they are quite advanced:</p>

<ul>
  <li><a href="http://aclweb.org/anthology//P/P09/P09-1067.pdf">Use variational algorithms</a>.</li>
  <li><a href="http://aclweb.org/anthology//W/W09/W09-1114.pdf">Use Markov chain Monte Carlo algorithms</a>.</li>
</ul>

<p>But the sky’s the limit! There are many ways to decode.  You can
try anything you want as long as you follow the ground rules:</p>

<h2 id="ground-rules">Ground Rules</h2>

<ul>
  <li>Each group should submit using one person as the designated uploader.</li>
  <li>You must turn in three things:
    <ol>
      <li>The output from your decoder uploaded to the <a href="http://sfu-nlp-class.appspot.com">leaderboard submission site</a> using the instructions given above. You can upload new output as often as you like, up until the assignment deadline. The Submit button for showing the test set scores will be unavailable until after the homework deadline and grace days have passed.</li>
      <li>Your code. Each group should assign one member to upload the source code to <a href="https://courses.cs.sfu.ca">Coursys</a> as the submission for Homework 4. The code should be self-contained, self-documenting, and easy to use. It should use the same input and output assumptions of <code class="highlighter-rouge">default.py</code>.</li>
      <li>A clear, mathematical description of your algorithm and its motivation written in scientific style. This needn’t be long, but it should be clear enough that one of your fellow students could re-implement it exactly. Include the file for this writeup as part of the tarball or zip file you will upload to <a href="https://courses.cs.sfu.ca">Coursys</a>. Include also how your group divided up the work load and each group member’s contribution to the homework solution.</li>
    </ol>
  </li>
  <li>You cannot use data or code resources outside of what is provided to you. You can use NLTK but not the NLTK translation decoder implementation. You cannot use any public implemenation of decoding such as <code class="highlighter-rouge">moses</code>, <code class="highlighter-rouge">Joshua</code>  or <code class="highlighter-rouge">cdec</code> or any other open source decoder.</li>
  <li>For the written description of your algorithm, you can use plain ASCII but for math equations it is better to use either <a href="http://www.latex-project.org/">latex</a> or <a href="https://github.com/gettalong/kramdown">kramdown</a>.  Do <strong>not</strong> use any proprietary or binary file formats such as Microsoft Word.</li>
</ul>

<p>If you have any questions or you’re confused about anything, just ask.</p>

<h4 id="acknowledgements">Acknowledgements</h4>

<p>This assignment is adapted from the decoding homework developed
by <a href="http://cs.jhu.edu/~post/">Matt Post</a> and <a href="http://cs.jhu.edu/~alopez/">Adam
Lopez</a>.</p>


        </div>
      </div>

      <footer class="text-center text-muted">
        <hr/>
        Last updated September 06, 2016.<br/>
        Forked from the JHU MT class code on <a href="https://github.com/mt-class/jhu">github <i class="fa fa-github-alt"></i></a> by <a href="https://github.com/mjpost">Matt Post</a> and <a href="https://github.com/alopez">Adam Lopez</a>.<br/>
        <br/><br/>
      </footer>
    </div>

    <!-- Page content of course! -->
    <!-- JS and analytics only. -->
    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="./dist/js/bootstrap.js"></script>
    <script type="text/javascript">
      $(document).ready(function(){
        $("#homework").addClass("active");
      });
    </script>
  </body>
</html>
