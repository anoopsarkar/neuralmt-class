<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SFU NLP class: Project</title>

    <!-- CSS -->
    <link href="dist/css/bootstrap.min.css" rel="stylesheet">
    <link href="dist/css/bootstrap-glyphicons.css" rel="stylesheet">
    <link href="assets/css/nlp-class.css" rel="stylesheet">   
    <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">

    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>

    <!-- MathJax -->
    <script type="text/javascript"
      src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
  </head>
  <body>
    <a class="sr-only" href="#content">Skip to main content</a>

    <!-- Docs master nav -->
    <header class="navbar navbar-fixed-top navbar-default" role="banner">
      <div class="container">
        <div class="row">
        <div class="navbar-header">
          <button class="navbar-toggle" type="button" data-toggle="collapse" data-target=".navbar-collapse">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <ul class="nav navbar-nav">
            <li id="main_page"><a href="index.html" class="navbar-brand">Natural Language Processing</a></li>
          </ul>
        </div>
        <nav class="collapse navbar-collapse" role="navigation">
          <ul class="nav navbar-nav">
            <li id="syllabus"><a href="syllabus.html">Syllabus</a></li>
            <li id="homework">
              <a class="dropdown-toggle" data-toggle="dropdown" href="#">Homework <span class="caret"></span></a>
              <ol class="dropdown-menu">
                <li><a href="hw0.html">0. Setup</a></li>
                <li><a href="hw1.html">1. Word Segmentation</a></li>
                <li><a href="hw2.html">2. Phrasal Chunking</a></li>
                <li><a href="hw3.html">3. Word Alignment</a></li>
                <li><a href="hw4.html">4. Translation Decoding</a></li>
                <li><a href="hw5.html">5. Translation Reranking</a></li>
              </ol>
            </li>
            <li id="leaderboard"><a href="leaderboard.html">Leaderboard</a></li>
            <li id="project"><a href="project.html">Project</a></li>
            <li id="faq"><a href="faq.html">FAQ</a></li>
          </ul>
        </nav>
      </div>
      </div>
    </header>

    <div class="container">
      <div class="row">
        <div class="col-sm-2 hidden-sm hidden-xs">
          <a href="http://en.wikipedia.org/wiki/Tower_of_Babel_(M._C._Escher)"> 
          <img src="assets/img/escherbabel.jpg" class="img-responsive img-rounded" alt=""/>
          </a> 
          <span class="text-muted"><i>1928 woodcut by M. C. Escher showing the Tower of Babel.</i></span>
        </div>
        <div class="col-sm-10">
          <h2 id="final-project">Final Project</h2>

<p class="text-muted">Due on Tuesday, December 6, 2016</p>

<p>The final project is to translate from Chinese to English using the
machine translation system you have built over the course of the
semester. You should use the code you have written for your homework
assignments to build the translation system.</p>

<h2 id="files">Files</h2>

<p>In order to complete the project, you are provided with data files
to build a Chinese-English machine translation system. This data
has already been processed into alignments, phrase tables, etc. You
can use the processed data as given to you or generate your own
alignments, phrase tables, etc.</p>

<p>You are also provided with some Python programs that generate a
phrase table with typical feature values: <script type="math/tex">p(e|f), p(f|e), lex(e|f),
lex(f|e)</script> where <script type="math/tex">e</script> and <script type="math/tex">f</script> are phrases in the phrase table.</p>

<h3 id="data-files">Data Files</h3>

<p><strong>Warning: do not redistribute this data. SFU has a license to use this data from the Linguistic Data Consortium (LDC) but we cannot take this data and give it to others.</strong></p>

<p>The data files are available on CSIL Linux machines in the following directory:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>/usr/shared/CMPT/nlp-class/project
</code></pre>
</div>

<p>Important information about the license is given in the file <code class="highlighter-rouge">LICENSE</code>.</p>

<h4 id="training-data">Training data</h4>

<p>The training data is taken from the following sources:</p>

<ul>
  <li><a href="https://catalog.ldc.upenn.edu/LDC2004T08">Hong Kong Parliament parallel corpus</a></li>
  <li><a href="https://catalog.ldc.upenn.edu/LDC2009T15">GALE Phase-1 Chinese newsgroup data</a>.</li>
</ul>

<p>Training data comes in different sizes. The data files in each of the
large, medium, and small folders are:</p>

<ul>
  <li><code class="highlighter-rouge">train.cn</code>: segmented Chinese corpus</li>
  <li><code class="highlighter-rouge">train.cn.unseg</code>: un-segmented Chinese corpus</li>
  <li><code class="highlighter-rouge">train.en</code>: lower-cased English corpus</li>
  <li><code class="highlighter-rouge">phrase-table/moses/phrase-table.gz</code>: phrase-table in the usual format
compatible with the <a href="http://statmt.org/moses/">Moses SMT system</a></li>
</ul>

<h5 id="toy">Toy</h5>

<p>First 2k sentences from the full training data.</p>

<h5 id="small">Small</h5>

<p>First 20k sentences from the full training data.</p>

<h5 id="medium">Medium</h5>

<p>First 100k sentences from the full training data.</p>

<h5 id="large">Large:</h5>

<p>The entire training data (2.3M sentences).</p>

<p>In the <code class="highlighter-rouge">large</code> directory, there are a few additional files:</p>

<ul>
  <li><code class="highlighter-rouge">phrase-table/dev-filtered/rules_cnt.final.out</code>: phrase table
filtered for the data in <code class="highlighter-rouge">dev</code>, so that only the phrases useful
for tuning your SMT system are in this phrase table.</li>
  <li><code class="highlighter-rouge">phrase-table/test-filtered/rules_cnt.final.out</code>: phrase table
filtered for the data in <code class="highlighter-rouge">test</code>.</li>
  <li><code class="highlighter-rouge">lex.e2f</code> and <code class="highlighter-rouge">lex.f2e</code>: lexical probabilities</li>
</ul>

<h4 id="tuning-set">Tuning set</h4>

<p>The files for tuning your SMT system are in the <code class="highlighter-rouge">dev</code> directory. This data
is meant to be used for tuning the weights of your machine translation
log-linear model. There are four references for each source sentence.</p>

<p>The data comes from the following sources:</p>

<ul>
  <li><a href="https://catalog.ldc.upenn.edu/LDC2002T01">Multiple-Translation Chinese (MTC) part 1</a></li>
  <li><a href="https://catalog.ldc.upenn.edu/LDC2004T07">Multiple-Translation Chinese (MTC) part 3</a></li>
</ul>

<h4 id="test-set">Test set</h4>

<p>The files that are used as test data to report your performance are in
the <code class="highlighter-rouge">test</code> directory. There are four references for each source sentence.</p>

<p>The data comes from the following source:</p>

<ul>
  <li><a href="https://catalog.ldc.upenn.edu/LDC2006T04">Multiple-Translation Chinese (MTC) part 4</a></li>
</ul>

<h4 id="language-model">Language Model</h4>

<p>The language model files are in the <code class="highlighter-rouge">lm</code> directory.</p>

<ul>
  <li><code class="highlighter-rouge">en.gigaword.3g.arpa.gz</code>: large LM estimated using
Kneser-Ney smoothing from the <a href="https://catalog.ldc.upenn.edu/LDC2011T07">English Gigaword corpus</a></li>
  <li><code class="highlighter-rouge">en.gigaword.3g.filtered.dev_test.arpa.gz</code>: small-sized LM filtered from the
large LM for the dev and test files (52MB compressed)</li>
  <li><code class="highlighter-rouge">en.gigaword.3g.filtered.train_dev_test.arpa.gz</code>: medium-sized LM filtered from the
large LM for the target side of the large phrase table and the dev and test files (93MB compressed)</li>
  <li><code class="highlighter-rouge">en.tiny.3g.arpa</code>: tiny LM from the decoding homework</li>
</ul>

<h4 id="chinese-word-segmentation-data">Chinese word segmentation data</h4>

<p>The training data for training a Chinese word segmenter is in the
<code class="highlighter-rouge">seg</code> directory.</p>

<p>The data comes from the <a href="http://www.sighan.org/bakeoff2005/">Chinese Word Segmentation Bakeoff</a>.</p>

<p>The data has been processed into a column format similar to the
chunking format used in the chunker homework. If you wish to convert
this into a frequency dictionary you will have to parse these column
files into words using the <code class="highlighter-rouge">B</code> (begin word) and <code class="highlighter-rouge">I</code> (inside word)
and <code class="highlighter-rouge">O</code> (single character word) tags.</p>

<p>There are three files:</p>

<ul>
  <li><code class="highlighter-rouge">cityu_train.utf8</code>: from the City University of Hong Kong</li>
  <li><code class="highlighter-rouge">msr_train.utf8</code>: from Microsoft Research (Beijing)</li>
  <li><code class="highlighter-rouge">upenn_train.utf8</code>: from the University of Pennsylvania Chinese Treebank</li>
</ul>

<p>They can be converted into a single frequency dictionary for your
homework segmenter, or you can use your chunker to train a context-aware
model of Chinese word segmentation.</p>

<p>You may want to check if your segmentation actually improves alignment
scores before you proceed through the entire translation pipeline.
A simple way to check this is to compare the IBM Model 1 scores
with the provided segmented Chinese aligned to the given parallel
English data with your own segmentation of the Chinese data also
aligned to the same parallel English data.</p>

<h3 id="programs-for-phrase-table-generation">Programs for phrase table generation</h3>

<p>The following are scripts that can be used to create a phrase table
with feature values from source, target and alignment data.</p>

<ul>
  <li><code class="highlighter-rouge">pp_xtrct_sc.sh</code>: shell script to run phrase extractor on the toy
data set. <strong>You should use this script if you have produced your
own alignments or if you have produced your own Chinese segmentation
and alignments based on that segmentation</strong></li>
  <li><code class="highlighter-rouge">pp_xtrct.sh</code>: shell script to run phrase extractor. It splits
the data into shards of 20K sentences each and then runs phrase
extraction in parallel. It then filters each phrase file for <code class="highlighter-rouge">dev</code>
or <code class="highlighter-rouge">test</code> and finally merges the phrases for each shard. This
script assumes you have access to a large cluster of multiple
machines managed by grid management software such as Torque (using
the <code class="highlighter-rouge">qsub</code> command).</li>
</ul>

<p>The scripts above call the following Python programs in the appropriate
sequence.</p>

<ul>
  <li><code class="highlighter-rouge">PPXtractor_ph1.py</code>: python program for extracting phrase-pairs from
a source, target, and alignment files.</li>
  <li><code class="highlighter-rouge">PPXtractor_ph2n3.py</code>: python program for identifying the source
phrases in the given dev/test set and filter the phrase file for the
source phrases.</li>
  <li><code class="highlighter-rouge">PPXtractor_ph2.py</code>: python program for computing forward and reverse
lexical scores.</li>
  <li><code class="highlighter-rouge">PPXtractor_ph3.py</code>: python program for estimating the forward
<script type="math/tex">P(s|t)</script> and reverse <script type="math/tex">P(t|s)</script> probabilities using relative frequency
estimation.</li>
</ul>

<h2 id="the-challenge">The Challenge</h2>

<p>The challenge is to use the code you have written for your homework
assignments in this course in order to build a Chinese-English
translation system and get a competitive BLEU score on the test
set.</p>

<p>You can choose to follow any route as long as you have a Chinese-English
machine translation system at the end. You should have a comparison
between at least two methods on the dataset, and compare their BLEU
scores.</p>

<p>Below I describe a few basic steps you can take in order to obtain
a performant Chinese-English SMT system using your homework code
from this course.</p>

<h3 id="get-decoder-working-on-toy-data">Get decoder working on toy data</h3>

<p>The absolute first step: get your decoder working with the <code class="highlighter-rouge">toy</code>
phrase table data and the tiny LM data from the <code class="highlighter-rouge">lm</code> directory.</p>

<p>Run your decoder on the test set and try to get a BLEU score.
It will have a terrible score but it will be a start.</p>

<h3 id="use-filtered-phrase-tables-with-your-decoder">Use filtered phrase tables with your decoder</h3>

<p>The most basic Chinese-English translator you can build is to use
the filtered phrase tables and language model with your decoder. I
recommend using the following data files:</p>

<ul>
  <li><code class="highlighter-rouge">large/phrase-table/test-filtered/rules_cnt.final.out</code>: phrase
table filtered for the data in <code class="highlighter-rouge">test</code>.</li>
  <li><code class="highlighter-rouge">en.gigaword.3g.filtered.train_dev_test.arpa.gz</code>: medium-sized
LM filtered from the large LM for the target side of the large
phrase table and the dev and test files (93MB compressed)</li>
  <li>The test files in the <code class="highlighter-rouge">test</code> directory.</li>
</ul>

<p>You need to change a few things in your decoder:</p>

<ul>
  <li>Modify the code that loads up the language model to read a gzip
file using the Python gzip module.</li>
  <li>Modify the decoder to use all the feature values in the phrase
table file (earlier we just had one feature per phrase pair, now
we have four). You can use uniform weights to combine the feature
values and the language model feature.</li>
  <li>Optionally use multiple references in computing the BLEU score
on the test set.</li>
</ul>

<p>This will give you a basic Chinese-English SMT system and would be
a valid course project submission. However, you can do a little bit
more to improve your performance.</p>

<h3 id="feedback-loop-between-decoder-and-reranker">Feedback loop between Decoder and Reranker</h3>

<p>Once you have your decoder working and producing a BLEU score
on the test set, you can then learn a better weight vector
using your reranking code.</p>

<ul>
  <li>Take the decoder from the previous section.</li>
  <li>Use the phrase table filtered for the data in <code class="highlighter-rouge">dev</code>:
<code class="highlighter-rouge">large/phrase-table/dev-filtered/rules_cnt.final.out</code> and produce
a 100-best list from the last stack in your decoder (the last stack
covers all input words).</li>
  <li>Use your reranking code to produce a weight vector that favors
translations that match the <code class="highlighter-rouge">dev</code> references.</li>
  <li>Use this new weight vector with your decoder to produce
a new 100-best list for the <code class="highlighter-rouge">dev</code> set. Iterate until improvement
in the BLEU score on the <code class="highlighter-rouge">dev</code> set is minimal or a small number 
of iterations (say 5) if it is taking too long to converge.</li>
  <li>Use the final weight vector with your decoder on the 
test set using the phrase table filtered for the test set: 
<code class="highlighter-rouge">phrase-table/test-filtered/rules_cnt.final.out</code>.</li>
</ul>

<p>Compare your BLEU score on the test set with your previous
approach that used uniform weights.</p>

<p>There are many other ideas and you should come up with some
ideas on your own.</p>

<h3 id="other-ideas">Other ideas</h3>

<ul>
  <li>Add new features to the decoder.  You can experiment with 
the features you added in your decoder and reranker homeworks.</li>
  <li>Improving Chinese segmentation using the <code class="highlighter-rouge">seg</code> data. However,
this approach has a serious penalty of having to produce new
alignments and subsequently a new phrase table. This is useful
only if you are certain your segmentations are going to be
better than we have provided. One approach is to produce 
<a href="ftp://ftp.cs.rochester.edu/pub/papers/ai/10.tr957.Bayesian_Learning_of_Tokenization_for_Machine_Translation.pdf">segmentations that are guaranteed to improve the alignment score</a>.</li>
  <li><a href="https://www.aclweb.org/anthology/I/I08/I08-1030.pdf">Improve handling of unknown Chinese words</a></li>
  <li><a href="http://www.phontron.com/paper/neubig12acl.pdf">Do not segment the Chinese sentence at all</a></li>
</ul>

<p>Ideas you should <strong>not</strong> try:</p>

<ul>
  <li>Using your own aligner and creating a phrase table. The alignments
we have provided to you are very high quality, for the particular
Chinese segmentations given to you. Simply getting new alignments
on the same data is unlikely to improve results.</li>
</ul>

<p>But the skyâ€™s the limit! You can try anything you want, as long
as you follow the ground rules.</p>

<h2 id="ground-rules">Ground Rules</h2>

<ul>
  <li>Each group should submit using one person as the designated uploader.</li>
  <li>You must turn in two things:
    <ol>
      <li>A write-up that explains what you accomplished in your course project. It should be written in a clear, scientific style and contain enough information for another student to replicate your results. More information about the write-up is given below.</li>
      <li>Your code. The code should be self-contained, self-documenting, and easy to use. Please include a detailed description of how to run your code.</li>
    </ol>
  </li>
  <li>Each group should assign one member to upload the write-up and source code to <a href="https://courses.cs.sfu.ca">Coursys</a> as a single tarball or zip files as the submission for Final Project.</li>
  <li>You cannot use data or code resources outside of what is provided to you. You can use NLTK but not any NLTK reranking code directly. You cannot use any public implemenation of reranking such as the one included in <code class="highlighter-rouge">moses</code>, <code class="highlighter-rouge">Joshua</code>  or <code class="highlighter-rouge">cdec</code> or any other open source decoder.</li>
</ul>

<h3 id="write-up">Write-up</h3>

<p>Your write-up is the most important part of your project. It must have the following sections:</p>

<ul>
  <li>Motivation
    <ul>
      <li>Which aspect of the translation system did your group choose to improve and reasons for your choice.</li>
    </ul>
  </li>
  <li>Approach
    <ul>
      <li>Describe the algorithms and machine learning models used in your project. Use a clear mathematical style to explain your model(s).</li>
    </ul>
  </li>
  <li>Data
    <ul>
      <li>Exactly which data files were used; also include here any external data that was not provided to you.</li>
    </ul>
  </li>
  <li>Code
    <ul>
      <li>Describe which homework code you used in your project. Provide exactly which code was used in your project not written by your group (e.g. use of an aligner from an open-source project).</li>
    </ul>
  </li>
  <li>Experimental Setup
    <ul>
      <li>Describe what kind of evaluation you are doing and which methods you are comparing against each other.</li>
    </ul>
  </li>
  <li>Results
    <ul>
      <li>Include a detailed comparison of different methods.</li>
    </ul>
  </li>
  <li>Analysis of the Results
    <ul>
      <li>Did you improve over the baseline. Why or why not?</li>
    </ul>
  </li>
  <li>Future Work
    <ul>
      <li>What could be fixed in your approach. What you did not have time to finish, but you think would be a useful addition to your project.</li>
    </ul>
  </li>
</ul>

<p>For your write-up, you can use plain ASCII but for math equations
and tables it is better to use either
<a href="http://www.latex-project.org/">latex</a> or
<a href="https://github.com/gettalong/kramdown">kramdown</a>.  Do <strong>not</strong> use
any proprietary or binary file formats such as Microsoft Word.</p>

<h2 id="grading-system">Grading system</h2>

<p>The final projects will be graded using the following criteria:</p>

<ul>
  <li>Originality</li>
  <li>Substance (amount of work done for the project)</li>
  <li>Well documented use of prior results from research papers</li>
  <li>Clarity of the writing</li>
  <li>Quality of experimental design</li>
  <li>Quality of evaluation and results</li>
  <li>(Re)Use of existing homework code</li>
  <li>Overall score (based on the above criteria)</li>
</ul>


        </div>
      </div>

      <footer class="text-center text-muted">
        <hr/>
        Last updated September 06, 2016.<br/>
        Forked from the JHU MT class code on <a href="https://github.com/mt-class/jhu">github <i class="fa fa-github-alt"></i></a> by <a href="https://github.com/mjpost">Matt Post</a> and <a href="https://github.com/alopez">Adam Lopez</a>.<br/>
        <br/><br/>
      </footer>
    </div>

    <!-- Page content of course! -->
    <!-- JS and analytics only. -->
    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="./dist/js/bootstrap.js"></script>
    <script type="text/javascript">
      $(document).ready(function(){
        $("#project").addClass("active");
      });
    </script>
  </body>
</html>
