<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SFU Neural MT class: Competitive Grammar Writing</title>

    <!-- CSS -->
    <link href="dist/css/bootstrap.min.css" rel="stylesheet">
    <link href="dist/css/bootstrap-glyphicons.css" rel="stylesheet">
    <link href="assets/css/nlp-class.css" rel="stylesheet">   
    <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">

    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>

    <!-- MathJax -->
    <script type="text/javascript"
      src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
  </head>
  <body>
    <a class="sr-only" href="#content">Skip to main content</a>

    <!-- Docs master nav -->
    <header class="navbar navbar-fixed-top navbar-default" role="banner">
      <div class="container">
        <div class="row">
        <div class="navbar-header">
          <button class="navbar-toggle" type="button" data-toggle="collapse" data-target=".navbar-collapse">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <ul class="nav navbar-nav">
            <li id="main_page"><a href="index.html" class="navbar-brand">Neural MT</a></li>
          </ul>
        </div>
        <nav class="collapse navbar-collapse" role="navigation">
          <ul class="nav navbar-nav">
            <li id="syllabus"><a href="syllabus.html">Syllabus</a></li>
            <li id="homework">
              <a class="dropdown-toggle" data-toggle="dropdown" href="#">Project <span class="caret"></span></a>
              <ol class="dropdown-menu">
                <li><a href="project0.html">1. Neural LM</a></li>
                <li><a href="project1.html">2. Perceptron MT</a></li>
                <li><a href="project2.html">3. Encoder-Decoder</a></li>
                <li><a href="project.html">4. Final Project</a></li>
              </ol>
            </li>
            <li id="bib"><a href="bibliography.html">Bibliography</a></li>
            <li id="faq"><a href="faq.html">FAQ</a></li>
          </ul>
        </nav>
      </div>
      </div>
    </header>

    <div class="container">
      <div class="row">
        <div class="col-sm-2 hidden-sm hidden-xs">
          <a href="http://en.wikipedia.org/wiki/Center_embedding"> 
          <img src="assets/img/embedding.jpg" class="img-responsive img-rounded" alt=""/>
          </a> 
          <span class="text-muted"><i>The Embedding introduces a strange form of language whose grammar can be 'self-embedded' by computers.</i></span>
        </div>
        <div class="col-sm-10">
          <h1 id="competitive-grammar-writing-in-python">Competitive Grammar Writing in Python</h1>

<p>Get started:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>git clone https://github.com/anoopsarkar/cgw.git
</code></pre>
</div>

<p>This task involves writing or creating weighted context-free grammars
in order to parse English sentences and utterances. The vocabulary
is fixed.</p>

<h2 id="notation">Notation</h2>

<p>A context-free grammar (CFG) is defined using the following building blocks:</p>

<ul>
  <li><script type="math/tex">N</script>, a set of non-terminal symbols (these symbols do not appear in the input)</li>
  <li><script type="math/tex">S</script>, one non-terminal from <script type="math/tex">N</script> called the start symbol. All derivations in a CFG start from <script type="math/tex">S</script></li>
  <li><script type="math/tex">V</script>, a vocabulary of words called terminal symbols. <script type="math/tex">N</script> and <script type="math/tex">V</script> are disjoint</li>
  <li>Rules of the form: <script type="math/tex">A \rightarrow \alpha</script> where <script type="math/tex">A \in N</script> and <script type="math/tex">\alpha \in (N \cup V)^\ast</script>.</li>
  <li>Weights or frequencies or probabilities can be associated with each rule in a CFG.</li>
  <li>A probabilistic CFG is defined as a group of conditional probabilities <script type="math/tex">P(\alpha \mid A)</script>: one for each non-terminal <script type="math/tex">A</script></li>
</ul>

<p>A context-free grammar that is in extended Chomsky Normal Form
(eCNF) iff the right hand side of each CFG rule is either one
non-terminal, or two non-terminals, or one terminal symbol.</p>

<p>This is a grammar in a formal sense. Just like we can write a
grammar for the syntax of Python, for instance. In this exercise
we will try to write a grammar for a fragment of English.</p>

<p>A derivation of this CFG starts with a string (called a sentential
form) containing the start symbol <script type="math/tex">S</script> and then replaces the
non-terminals in that string recursively with the right hand side
of a rule (if there are multiple right hand sides for the same
non-terminal we pick one of them) until only terminal symbols are
left in the string. This sequence of terminal symbols is a valid
string in the (formal) language generated by the CFG.</p>

<h2 id="the-data">The Data</h2>

<p>Initial versions of the context-free grammar files are provided to you:</p>

<ul>
  <li><code class="highlighter-rouge">S1.gr</code>: the default grammar file contains a context-free grammar in eCNF.</li>
  <li><code class="highlighter-rouge">S2.gr</code>: the default backoff grammar file.</li>
  <li><code class="highlighter-rouge">Vocab.gr</code>: the vocabulary file contains rules of the type <code class="highlighter-rouge">A -&gt; a</code> where <code class="highlighter-rouge">A</code> is a non-terminal that represents the part of speech and <code class="highlighter-rouge">a</code> is a word (also called a terminal symbol).</li>
</ul>

<p>Here is a fragment of <code class="highlighter-rouge">S1.gr</code>. Each line is a weighted context-free
grammar rule. First column is the weight, second column is the
left-hand side non-terminal of the CFG rule and the rest of the
line is the right-hand side of the CFG rule:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>1   S1   NP VP
1   S1   NP _VP
1   _VP  VP Punc
20  NP   Det Nbar
1   NP   Proper
</code></pre>
</div>

<p>The non-terminal <code class="highlighter-rouge">VP</code> is used to keep the grammar in eCNF. The
probability of a particular rule is obtained by normalizing the
weights for each left-hand side non-terminal in the grammar. For
example, for rule <code class="highlighter-rouge">NP -&gt; Det Nbar</code> the conditional probability
<code class="highlighter-rouge">P(Det Nbar | NP)</code> is <script type="math/tex">\frac{20}{20+1}</script>.</p>

<p>The grammars in <code class="highlighter-rouge">S1.gr</code> and <code class="highlighter-rouge">S2.gr</code> are connected via the following rules in <code class="highlighter-rouge">S1.gr</code>:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>99 TOP  S1
1  TOP  S2
1  S2   Misc
</code></pre>
</div>

<h2 id="other-files">Other files</h2>

<ul>
  <li><code class="highlighter-rouge">allowed_words.txt</code>: This file contains all the words that are allowed. You should make sure that your grammar generates sentences using exactly the words in this file. It does not specify the part of speech for each word, so you can choose to model the ambiguity of words in terms of part of speech in the <code class="highlighter-rouge">Vocab.gr</code> file.</li>
  <li><code class="highlighter-rouge">example-sentences.txt</code>: This file contains example sentences that you can use as a starting point for your grammar development. Only the first two sentences of this file can be parsed using the default <code class="highlighter-rouge">S1.gr</code> grammar. The rest are parsed with the backoff <code class="highlighter-rouge">S2.gr</code> grammar.</li>
  <li><code class="highlighter-rouge">unseen.tags</code>: Used to deal with unknown words. You should not have to use this file during parsing, but the parser provided to you can optionally use this file in order to deal with unknown words in the input.</li>
</ul>

<h2 id="the-parser-and-generator">The Parser and Generator</h2>

<p>You are given a parser that takes sentences as input and produces
parse trees and also a generator which generates a random sample
of sentences from the weighted grammar. Parsing and generating will
be useful steps in your grammar development strategy. You can learn
the various options for running the parser and generator using the
following command.</p>

<p>The parser has several options to speed up parsing, such as beam
size and pruning. Most likely you will not need to use those options
(unless your grammars are huge).</p>

<div class="highlighter-rouge"><pre class="highlight"><code>python pcfg_parse_gen.py -h
</code></pre>
</div>

<h3 id="parsing-input">Parsing input</h3>

<p>The parser provided to you reads in the grammar files and a set of
input sentences. It prints out the single most probable parse tree
for each sentence (using the weights assigned to each rule in the
input context-free grammar).</p>

<p>For example given the input sentence <code class="highlighter-rouge">Arthur is the king</code> the parser
will return the most probable derivation of the sentence which uses
the following rules (shown with their probabilities) from the grammar
files:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>99/100 TOP    -&gt; S1
1/2    S1     -&gt; NP _VP
1/21   NP     -&gt; Proper
1/9    Proper -&gt; Arthur
1      _VP    -&gt; VP Punc
1      VP     -&gt; VerbT NP
1/6    VerbT  -&gt; is
20/21  NP     -&gt; Det Nbar
1/9    Det    -&gt; the
10/11  Nbar   -&gt; Noun
1/21   Noun   -&gt; king
</code></pre>
</div>

<p>There might be many other derivations for this input string but the
derivation (which is just a list of CFG rules that fit together to
derive the input string) shown above is the most probable one
returned by the Python program provided to you using an algorithm
called the CKY algorithm which returns the argmax derivation for
any input string.</p>

<p>The probability of the derivation is simply the product of the
probabilities of the rules used in that derivation. For this
derivation the probability is:</p>

<p>$$ \frac{99}{100} \times \frac{1}{2} \times \frac{1}{21} \times \frac{1}{9} \times 1 \times 1 \times \frac{1}{6} \times \frac{20}{21} \times \frac{1}{9} \times \frac{10}{11} \times \frac{1}{21} $$</p>

<p>The derivation can be written down as a parse tree by simply linking
the non-terminals together. The following tree is simply another
(more graphical) way to represent the derivation shown above.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>(TOP (S1 (NP (Proper Arthur) ) 
         (_VP (VP (VerbT is) 
                  (NP (Det the) 
                      (Nbar (Noun king) ))) 
              (Punc .))) )
</code></pre>
</div>

<p>The parser also reports the negative
cross-entropy score for the whole set of sentences. Assume the
parser gets a text of <script type="math/tex">n</script> sentences to parse: <script type="math/tex">s_1, s_2, \ldots,
s_n</script> and we write <script type="math/tex">|s_i|</script> to denote the length of each sentence
<script type="math/tex">s_i</script>. The probability assigned to each sentence by the parser is
<script type="math/tex">P(s_1), P(s_2), \ldots, P(s_n)</script>. The negative cross entropy is the
average log probability score (bits per word) and is defined as
follows:</p>

<p>$$\textrm{score}(s_1, \ldots, s_n) = \frac{ \log P(s_1) + \log P(s_2) + \ldots + \log P(s_n) }{ |s_1| + |s_2| + \ldots + |s_n| }$$</p>

<p>We keep the value as negative cross entropy so that higher scores
are better.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>python pcfg_parse_gen.py -i -g "*.gr" &lt; example_sentences.txt
#loading grammar files: S1.gr, S2.gr, Vocab.gr
#reading grammar file: S1.gr
#reading grammar file: S2.gr
#reading grammar file: Vocab.gr

... skipping the parse trees ...

#-cross entropy (bits/word): -10.0502
</code></pre>
</div>

<h3 id="generating-output">Generating output</h3>

<p>In order to aid your grammar development you can also generate
sentences from the weighted grammar to test if your grammar is
producing grammatical sentences with high probability. The following
command samples 20 sentences from the <code class="highlighter-rouge">S1.gr,Vocab.gr</code> grammar
files.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>python pcfg_parse_gen.py -o 20 -g S1.gr,Vocab.gr
#loading grammar files: S1.gr, Vocab.gr
#reading grammar file: S1.gr
#reading grammar file: Vocab.gr
every pound covers this swallow
no quest covers a weight
Uther Pendragon rides any quest
the chalice carries no corner .
any castle rides no weight
Sir Lancelot carries the land .
a castle is each land
every quest has any fruit .
no king carries the weight
that corner has every coconut
the castle is the sovereign
the king has this sun
that swallow has a king
another story rides no story
this defeater carries that sovereign
each quest on no winter carries the sovereign .
another king has no coconut through another husk .
a king rides another winter
that castle carries no castle
every horse covers the husk .
</code></pre>
</div>

<h2 id="acknowledgements">Acknowledgements</h2>

<p>The idea for this task and the original data files are taken from the following paper:</p>

<blockquote>
  <p>Jason Eisner and Noah A. Smith. <a href="http://aclweb.org/anthology/W/W08/W08-0212.pdf">Competitive Grammar Writing</a>. In Proceedings of the ACL Workshop on Issues in Teaching Computational Linguistics, pages 97-105, Columbus, OH, June 2008.</p>
</blockquote>


        </div>
      </div>

      <footer class="text-center text-muted">
        <hr/>
        Last updated September 06, 2016.<br/>
        Forked from the JHU MT class code on <a href="https://github.com/mt-class/jhu">github <i class="fa fa-github-alt"></i></a> by <a href="https://github.com/mjpost">Matt Post</a> and <a href="https://github.com/alopez">Adam Lopez</a>.<br/>
        <br/><br/>
      </footer>
    </div>

    <!-- Page content of course! -->
    <!-- JS and analytics only. -->
    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="./dist/js/bootstrap.js"></script>
    <script type="text/javascript">
      $(document).ready(function(){
        $("#homework").addClass("active");
      });
    </script>
  </body>
</html>
