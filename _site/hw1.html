<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SFU NLP class: Homework 1 | Segmentation</title>

    <!-- CSS -->
    <link href="dist/css/bootstrap.min.css" rel="stylesheet">
    <link href="dist/css/bootstrap-glyphicons.css" rel="stylesheet">
    <link href="assets/css/nlp-class.css" rel="stylesheet">   
    <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">

    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>

    <!-- MathJax -->
    <script type="text/javascript"
      src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
  </head>
  <body>
    <a class="sr-only" href="#content">Skip to main content</a>

    <!-- Docs master nav -->
    <header class="navbar navbar-fixed-top navbar-default" role="banner">
      <div class="container">
        <div class="row">
        <div class="navbar-header">
          <button class="navbar-toggle" type="button" data-toggle="collapse" data-target=".navbar-collapse">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <ul class="nav navbar-nav">
            <li id="main_page"><a href="index.html" class="navbar-brand">Natural Language Processing</a></li>
          </ul>
        </div>
        <nav class="collapse navbar-collapse" role="navigation">
          <ul class="nav navbar-nav">
            <li id="syllabus"><a href="syllabus.html">Syllabus</a></li>
            <li id="homework">
              <a class="dropdown-toggle" data-toggle="dropdown" href="#">Homework <span class="caret"></span></a>
              <ol class="dropdown-menu">
                <li><a href="hw0.html">0. Setup</a></li>
                <li><a href="hw1.html">1. Word Segmentation</a></li>
                <li><a href="hw2.html">2. Phrasal Chunking</a></li>
                <li><a href="hw3.html">3. Word Alignment</a></li>
                <li><a href="hw4.html">4. Translation Decoding</a></li>
                <li><a href="hw5.html">5. Translation Reranking</a></li>
              </ol>
            </li>
            <li id="leaderboard"><a href="leaderboard.html">Leaderboard</a></li>
            <li id="project"><a href="project.html">Project</a></li>
            <li id="faq"><a href="faq.html">FAQ</a></li>
          </ul>
        </nav>
      </div>
      </div>
    </header>

    <div class="container">
      <div class="row">
        <div class="col-sm-2 hidden-sm hidden-xs">
          <a href="http://en.wikipedia.org/wiki/Text_segmentation"> 
          <img src="assets/img/mayhem.jpg" class="img-responsive img-rounded" alt=""/>
          </a> 
          <span class="text-muted"><i>Segmentation is harder than it seems.</i></span>
        </div>
        <div class="col-sm-10">
          <h1 id="word-segmentation-span-classtext-mutedhomework-1span">Word Segmentation <span class="text-muted">Homework 1</span></h1>

<p class="text-muted">Due on Tuesday, September 27, 2016</p>

<p>Word segmentation is the task of restoring missing word
boundaries. For example, in some cases word boundaries
are lost as in web page URLs like <em>choosespain.com</em> which
could be either <em>chooses pain</em> or <em>choose spain</em> and you 
might visit such an URL looking for one or the other.</p>

<p>This homework is on Chinese word segmentation, a language
in which word boundaries are not usually provided. For
instance here is an example Chinese sentence without word
boundaries:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>北京大学生比赛
</code></pre>
</div>

<p>This can be segmented a few different ways and one segmentation
leads to a particular meaning (indicated by the English translation below):</p>

<div class="highlighter-rouge"><pre class="highlight"><code>北京 大学生 比赛
Beijing student competition
</code></pre>
</div>

<p>A different segmentation leads to a different meaning (and translation):</p>

<div class="highlighter-rouge"><pre class="highlight"><code>北京大学 生 比赛
Peking University Health Competition
</code></pre>
</div>

<p>We will be using <em>training data</em> collected from Chinese
sentences that have been segmented by human experts.
We will run the word segmentation program that you
will write for this homework on <em>test data</em> that will
be automatically evaluated against a reference
segmentation.</p>

<h2 id="getting-started">Getting Started</h2>

<p>You must have git and python (2.7) on your system to run the assignments.
Once you’ve confirmed this, run this command:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>git clone https://github.com/anoopsarkar/nlp-class-hw.git
</code></pre>
</div>

<p>In the <code class="highlighter-rouge">segmenter</code> directory you will find a python program called
<code class="highlighter-rouge">default.py</code>, which contains a complete but very simple segmentation algorithm.
It simply inserts word boundaries between each Chinese character in the
input. It is a terrible segmenter but it does read the input and produce
a valid output that can be scored.</p>

<p>You can see how well <code class="highlighter-rouge">default.py</code> does by running the following:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>python default.py | python score-segments.py
</code></pre>
</div>

<p>Alternatively, you can run:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>python default.py &gt; output
python score-segments.py -t output
</code></pre>
</div>

<p>The score reported is <a href="http://en.wikipedia.org/wiki/F1_score">F-measure</a> which combines 
<a href="http://en.wikipedia.org/wiki/Precision_and_recall">precision and recall</a> into a single score.</p>

<h2 id="the-challenge">The Challenge</h2>

<p>Your task is to <em>improve the F-measure as much as possible</em>. To help you do
this the <code class="highlighter-rouge">data</code> directory in <code class="highlighter-rouge">segmenter</code> contains two files:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>count_1w.txt : unigram counts of Chinese words
count_2w.txt : bigram counts of Chinese word pairs
</code></pre>
</div>

<h3 id="the-baseline">The Baseline</h3>

<p>A simple baseline uses a unigram language model over Chinese words.
The input is a sequence of Chinese characters (without word
boundaries): <script type="math/tex">c_0, \ldots, c_n</script>.</p>

<p>Let us define a word as a sequence of characters: <script type="math/tex">w_i^j</script> is
a word that spans from character <script type="math/tex">i</script> to character <script type="math/tex">j</script>. So
one possible word sequence is <script type="math/tex">w_0^3 w_4^{10} w_{11}^n</script>. We
can score this sequence using unigram probabilities.</p>

<p>$$\arg\max_{w_0^i, w_{i+1}^j, \ldots, w_{n-k}^n} P_w(w_0^i) \times P_w(w_{i+1}^j) \times \ldots \times P_w(w_{n-k}^n)$$</p>

<p>The unigram probability <script type="math/tex">P_w</script> can be constructed using the data
in <code class="highlighter-rouge">count_1w.txt</code>. The model is simple, an unigram model, but the
search is over all possible ways to form word sequences for the
input sequence of characters. The argmax over all such sequences
will give you the baseline system. The <script type="math/tex">\arg\max</script> above can be computed
using the following recursive search over <script type="math/tex">segment(c_0, \ldots, c_n)</script>:</p>

<p>$$\begin{eqnarray}
segment(c_i, \ldots, c_j) &amp;=&amp; \arg\max_{\forall k &lt;= L} P_w(w_i^k) \times segment(c_{k+1}, \ldots, c_j) \\
segment(\emptyset) &amp;=&amp; 1.0
\end{eqnarray}$$</p>

<p>where <script type="math/tex">L = min(maxlen, j)</script> in order to avoid considering segmentations
of very long words which are going to be very unlikely.
<script type="math/tex">segment(\emptyset)</script> is the base case of the recursion: an input
of length zero, which results in a segmentation of length zero with
probability <script type="math/tex">1.0</script>.</p>

<p>One can <a href="http://en.wikipedia.org/wiki/Memoization">memoize</a> <script type="math/tex">segment</script> in order
to avoid the slow exploration of the exponentially many segmentations.
An alternative is to do this iteratively. The following pseudo-code illustrates
how to find the argmax iteratively.</p>

<h4 id="algorithm-iterative-segmenter">Algorithm: Iterative segmenter</h4>

<hr />
<p><strong>## Data Structures ##</strong></p>

<dl class="dl-horizontal">
  <dt><code class="highlighter-rouge">input</code></dt>
  <dd>the input sequence of characters</dd>
  <dt><code class="highlighter-rouge">chart</code></dt>
  <dd>the dynamic programming table to store the argmax for every prefix of <code class="highlighter-rouge">input</code></dd>
  <dd>indexed by character position in <code class="highlighter-rouge">input</code></dd>
  <dt><code class="highlighter-rouge">Entry</code></dt>
  <dd>each entry in the <code class="highlighter-rouge">chart</code> has four components: Entry(<code class="highlighter-rouge">word</code>, <code class="highlighter-rouge">start-position</code>, <code class="highlighter-rouge">log-probability</code>, <code class="highlighter-rouge">back-pointer</code>)</dd>
  <dd>the <code class="highlighter-rouge">back-pointer</code> in each <code class="highlighter-rouge">entry</code> links it to a previous entry that it extends</dd>
  <dt><code class="highlighter-rouge">heap</code></dt>
  <dd>a list or priority queue containing the entries to be expanded, sorted on <code class="highlighter-rouge">start-position</code> or <code class="highlighter-rouge">log-probability</code></dd>
</dl>

<hr />
<p><strong>## Initialize the <code class="highlighter-rouge">heap</code> ##</strong></p>

<ul class="list-unstyled">
  <li>for each <code class="highlighter-rouge">word</code> that matches <code class="highlighter-rouge">input</code> at position 0
    <ul>
      <li>insert Entry(<code class="highlighter-rouge">word</code>, 0, <script type="math/tex">\log P_w</script>(<code class="highlighter-rouge">word</code>), <script type="math/tex">\emptyset</script>) into <code class="highlighter-rouge">heap</code></li>
    </ul>
  </li>
</ul>

<p><strong>## Iteratively fill in <code class="highlighter-rouge">chart[i]</code> for all <code class="highlighter-rouge">i</code> ##</strong></p>

<ul class="list-unstyled">
  <li>while <code class="highlighter-rouge">heap</code> is nonempty:
    <ul>
      <li><code class="highlighter-rouge">entry</code> = top entry in the <code class="highlighter-rouge">heap</code></li>
      <li>get the <code class="highlighter-rouge">endindex</code> based on the length of the word in <code class="highlighter-rouge">entry</code></li>
      <li>if <code class="highlighter-rouge">chart</code>[<code class="highlighter-rouge">endindex</code>] has a previous entry, <code class="highlighter-rouge">preventry</code>
        <ul>
          <li>if <code class="highlighter-rouge">entry</code> has a higher probability than <code class="highlighter-rouge">preventry</code>:
            <ul>
              <li><code class="highlighter-rouge">chart</code>[<code class="highlighter-rouge">endindex</code>] = <code class="highlighter-rouge">entry</code></li>
            </ul>
          </li>
          <li>if <code class="highlighter-rouge">entry</code> has a lower or equal probability than <code class="highlighter-rouge">preventry</code>:
            <ul>
              <li>continue  <strong>## we have already found a good segmentation until <code class="highlighter-rouge">endindex</code> ##</strong></li>
            </ul>
          </li>
        </ul>
      </li>
      <li>else
        <ul>
          <li><code class="highlighter-rouge">chart</code>[<code class="highlighter-rouge">endindex</code>] = <code class="highlighter-rouge">entry</code></li>
        </ul>
      </li>
      <li>for each <code class="highlighter-rouge">newword</code> that matches <code class="highlighter-rouge">input</code> starting at position <code class="highlighter-rouge">endindex</code>+1
        <ul>
          <li><code class="highlighter-rouge">newentry</code> = Entry(<code class="highlighter-rouge">newword</code>, <code class="highlighter-rouge">endindex</code>+1, <code class="highlighter-rouge">entry</code>.<code class="highlighter-rouge">log-probability</code> + <script type="math/tex">\log P_w</script>(<code class="highlighter-rouge">newword</code>), <code class="highlighter-rouge">entry</code>)</li>
          <li>if <code class="highlighter-rouge">newentry</code> does not exist in <code class="highlighter-rouge">heap</code>:
            <ul>
              <li>insert <code class="highlighter-rouge">newentry</code> into <code class="highlighter-rouge">heap</code></li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><strong>## Get the best segmentation ##</strong></p>

<ul class="list-unstyled">
  <li><code class="highlighter-rouge">finalindex</code> is the length of <code class="highlighter-rouge">input</code></li>
  <li><code class="highlighter-rouge">finalentry</code> = <code class="highlighter-rouge">chart</code>[<code class="highlighter-rouge">finalindex</code>]</li>
  <li>The best segmentation starts from <code class="highlighter-rouge">finalentry</code> and follows the <code class="highlighter-rouge">back-pointer</code> recursively until the first word</li>
</ul>
<hr />

<p>It might help to examine <a href="https://gist.github.com/anoopsarkar/da67c6566a7268bb53b7">an example
run</a> of
the above pseudo-code on a particular input. To keep the example
short, the segmenter in the example assumes that unknown words can
only be on length one. You will get a better F-score if you allow
unknown words of arbitrary length (with the appropriate smoothed
probability score).</p>

<h3 id="your-task">Your Task</h3>

<p>Developing a segmenter using the above pseudo-code that uses unigram probabilities is
good enough to get close to the baseline system. But getting closer to the oracle
score will be a more interesting challenge. To get full credit you
<strong>must</strong> experiment with at least one additional model of your
choice and document your work. Here are some ideas:</p>

<ul>
  <li>Use the bigram model to score word segmentation candidates.</li>
  <li>Do better <em>smoothing</em> of the unigram and bigram probability models.</li>
</ul>

<p>But the sky’s the limit! You are welcome to design your own model, as long 
as you follow the ground rules:</p>

<h2 id="ground-rules">Ground Rules</h2>

<ul>
  <li>Each group should submit using one person as the designated uploader.</li>
  <li>You must turn in three things:
    <ol>
      <li>A segmentation of the entire dataset which is in <code class="highlighter-rouge">segmenter/data/input</code> uploaded to the <a href="http://sfu-nlp-class.appspot.com">leaderboard submission site</a> according to <a href="hw0.html">the Homework 0 instructions</a>. You can upload new output as often
as you like, up until the assignment deadline. Your score on the leaderboard is the score on the development data set which shown to you immediately after you upload your output file. The <strong>Submit</strong> button in unavailable until after the homework deadline has passed, and when pressed it will show you the test data set score.
The output will be evaluated using a secret metric, but the <code class="highlighter-rouge">score-segments.py</code> program will give you a good
idea of how well you’re doing.</li>
      <li>Your code. Each group should assign one member to upload the source code to <a href="https://courses.cs.sfu.ca">Coursys</a> as the submission for Homework 1. The code should be self-contained, self-documenting, and easy to use. It should use the same input and output assumptions of <code class="highlighter-rouge">default.py</code>.</li>
      <li>A clear, mathematical description of your algorithm and its motivation
written in scientific style. This needn’t be long, but it should be
clear enough that one of your fellow students could re-implement it 
exactly. Include the file for this writeup as part of the tarball or zip file you
will upload to <a href="https://courses.cs.sfu.ca">Coursys</a>.</li>
    </ol>
  </li>
  <li>You cannot use data or code resources outside of what is provided
to you. You can use NLTK but not the NLTK tokenizer class.</li>
  <li>For the written description of your algorithm, you can use plain ASCII but
for math equations it is better to use either
<a href="http://www.latex-project.org/">latex</a> or
<a href="https://github.com/gettalong/kramdown">kramdown</a>.  Do <strong>not</strong> use
any proprietary or binary file formats such as Microsoft Word.</li>
</ul>

<p>If you have any questions or you’re confused about anything, just ask.</p>


        </div>
      </div>

      <footer class="text-center text-muted">
        <hr/>
        Last updated September 06, 2016.<br/>
        Forked from the JHU MT class code on <a href="https://github.com/mt-class/jhu">github <i class="fa fa-github-alt"></i></a> by <a href="https://github.com/mjpost">Matt Post</a> and <a href="https://github.com/alopez">Adam Lopez</a>.<br/>
        <br/><br/>
      </footer>
    </div>

    <!-- Page content of course! -->
    <!-- JS and analytics only. -->
    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="./dist/js/bootstrap.js"></script>
    <script type="text/javascript">
      $(document).ready(function(){
        $("#homework").addClass("active");
      });
    </script>
  </body>
</html>
